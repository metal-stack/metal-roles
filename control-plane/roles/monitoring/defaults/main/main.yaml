---
monitoring_namespace: monitoring
monitoring_grafana_admin_password: admin
monitoring_grafana_dashboard_timezone: Europe/Berlin
monitoring_grafana_additional_datasources:
  - name: Loki
    type: loki
    url: http://loki-stack:3100/
    access: proxy
    orgId: 1
    version: 1
    isDefault: false
    jsonData:
      maxLines: 1000
  - name: AlertManager
    type: camptocamp-prometheus-alertmanager-datasource
    url: http://prometheus-alertmanager:9093/
    access: proxy
    jsonData:
      severity_critical: "critical"
      severity_high: "high"
      severity_warning: "warning"
      severity_info: "info1"
monitoring_grafana_ingress_dns: "grafana.{{ metal_control_plane_ingress_dns }}"
monitoring_prometheus_ingress_dns: prometheus.{{ metal_control_plane_ingress_dns }}
monitoring_ingress_grafana_tls: yes
monitoring_prometheus_ingress_enabled: false
monitoring_additional_ingress_annotations: {}
  
# alertmanager
# monitoring_additional_alertmanager_annotations:
#   - config:
#       global:
#         resolve_timeout: 5m
#         slack_api_url: "{{ monitoring_slack_url }}"
#       route:
#         group_wait: 2m
#         group_interval: 15m
#         group_by: ['...']
#         repeat_interval: 120h
#         receiver: 'null'
#         routes:
#         - match_re:
#             severity: 'critical'
#           receiver: 'alertlogger'
#           continue: true
#         - match_re:
#             job: '.*'
#           receiver: 'alertlogger'
#           continue: true
#         - match_re:
#             pod: '(reserve-excess|Watchdog|stackdriver|heapster)'
#           receiver: 'null'
#         - match_re:
#             deployment: '(reserve-excess|stackdriver|heapster)'
#           receiver: 'null'
#         - receiver: 'zis-proxy'
#           matchers:
#             - severity="critical"
#           continue: true
#         - match_re:
#             severity: 'critical'
#           receiver: 'slack'
#           continue: true
#         - match_re:
#             severity: 'critical'
#           receiver: 'rocketchat'
#           continue: true
#     inhibit_rules:
#     - source_match:
#         alertname: 'Watchdog'
#       target_match_re:
#         pod: 'reserve-excess.*'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match_re:
#         pod: 'prometheus-to-sd.*'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match_re:
#         pod: 'metrics-server.*'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match_re:
#         pod: 'heapster.*'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match_re:
#         pod: 'stackdriver.*'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match:
#         deployment: '(reserve-excess|heapster|stackdriver)'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match:
#         alertname: 'KubeApiServerTooManyAuditlogFailures'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match:
#         alertname: 'PrometheusOutOfOrderTimestamps'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match:
#         alertname: '"KubeHpaMaxedOut"'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'Watchdog'
#       target_match:
#         alertname: 'etcdHighNumberOfFailedGRPCRequests'
#       equal: ['prometheus']
#     - source_match:
#         alertname: 'InternetUplinkDown'
#       target_match_re:
#         alertname: '(EndpointDown)'
#       equal: ['partition']
#     receivers:
#     - name: 'slack'
#       slack_configs:
#       - channel: {{ monitoring_slack_channel | b64decode }}
#         send_resolved: true
#         title: |-
# {%raw%}
#           {{ .GroupLabels.alertname }}
#           {{- if gt (len .CommonLabels) (len .GroupLabels) -}}
#             {{" "}}(
#             {{- with .CommonLabels.Remove .GroupLabels.Names }}
#               {{- range $index, $label := .SortedPairs -}}
#                 {{ if (and (ne $label.Name "mc_tool_rule") (and (ne $label.Name "job") (and (ne $label.Name "service") (and (ne $label.Name "endpoint") (ne $label.Name "prometheus"))))) -}}
#                   {{- if $index }}, {{ end -}}
#                   {{- $label.Name }}="{{ $label.Value -}}"
#                 {{- end }}
#               {{- end }}
#             {{- end -}}
#             )
#           {{- end }}
#         text: >-
#           {{ with index .Alerts 0 -}}
#             :chart_with_upwards_trend: *<{{ .GeneratorURL }}|Graph>*
#             {{- if .Annotations.runbook }}   :notebook: *<{{ .Annotations.runbook }}|Runbook>*{{ end }}
#           {{ end }}
#           *Alert details*:
#           {{ range .Alerts -}}
#             *Alert:* {{ .Labels.alertname }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
#           {{ if .Annotations.description }}*Description:* {{ .Annotations.description }}{{end}}
#           {{ if .Annotations.summary }}*Summary:* {{ .Annotations.summary }}{{end}}
#           *Details:*
#             {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`
#             {{ end }}
#           {{ end }}
# {%endraw%}
#     - name: 'null'
#     - name: 'alertlogger'
#       webhook_configs:
#         - url: http://alertlogger:5001
# metal metrics exporter
monitoring_metal_api_url: "http://metal-api.metal-control-plane.svc:8080/metal"
monitoring_metal_api_hmac: "metal-admin"

# rethinkdb exporter
monitoring_rethinkdb_exporter_metal_db_password: "change-me"
monitoring_rethinkdb_exporter_metal_db_endpoint: metal-db.metal-control-plane:28015
