---
prometheus:
  ingress:
    enabled: {{ monitoring_prometheus_ingress_enabled }}
    ingressClassName: nginx
    hosts:
      - {{ monitoring_prometheus_ingress_dns }}
{% if monitoring_additional_ingress_annotations %}
    annotations:
{% for key, value in monitoring_additional_ingress_annotations.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}

  prometheusSpec:
    additionalArgs:
      - name: "web.enable-remote-write-receiver"
        value: ""
{% if monitoring_prometheus_image_tag is defined %}
    image:
      tag: "{{ monitoring_prometheus_image_tag }}"
{% endif %}
{% if monitoring_prometheus_storage_spec is defined %}
    storageSpec:
      {{ monitoring_prometheus_storage_spec | indent(6) }}
{% endif %}
    thanos:
      image: "{{ thanos_image_name }}:{{ thanos_image_tag }}"
      version: "{{ thanos_image_tag }}"
{% if monitoring_thanos_object_store_config is defined %}
      objectStorageConfig:
        existingSecret:
          key: objstore.yml
          name: thanos-objstore-secret
{% endif %}
  thanosService:
    enabled: true

grafana:
  adminPassword: {{ monitoring_grafana_admin_password }}
  defaultDashboardsTimezone: {{ monitoring_grafana_dashboard_timezone }}

  additionalDataSources: {{ monitoring_grafana_additional_datasources | to_json }}
  grafana.ini:
    server:
{% if monitoring_ingress_grafana_tls %}
      root_url: https://{{ monitoring_grafana_ingress_dns }}
{% else %}
      root_url: http://{{ monitoring_grafana_ingress_dns }}
{% endif %}
{% if monitoring_grafana_github_oauth %}
    auth.github: {{ monitoring_grafana_github_oauth | to_json }}
{% endif %}
{% if monitoring_grafana_extra_secret_mounts %}
  extraSecretMounts: {{ monitoring_grafana_extra_secret_mounts | to_json }}
{% endif %}
  sidecar:
    datasources:
      uid: prometheus
      url: http://thanos-query-frontend:9090
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - {{ monitoring_grafana_ingress_dns }}
{% if monitoring_ingress_grafana_tls %}
    tls:
      - secretName: monitoring-ingress-tls
        hosts:
        - {{ monitoring_grafana_ingress_dns }}
{% endif %}
{% if monitoring_ingress_grafana_tls or monitoring_additional_ingress_annotations %}
    annotations:
{% endif %}
{% if monitoring_ingress_grafana_tls %}
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
{% endif %}
{% if monitoring_additional_ingress_annotations %}
{% for key, value in monitoring_additional_ingress_annotations.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}
  plugins:
    - camptocamp-prometheus-alertmanager-datasource

alertmanager:
  ingress:
    enabled: {{ monitoring_alertmanager_ingress_enabled }}
    ingressClassName: nginx
    hosts:
      - {{ monitoring_alertmanager_ingress_dns }}

{% if monitoring_alertmanager_ingress_tls %}
    tls:
      - secretName: monitoring-alertmanager-ingress-tls
        hosts:
        - {{ monitoring_alertmanager_ingress_dns }}
{% endif %}

    annotations:
{% if monitoring_alertmanager_ingress_tls %}
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
{% endif %}

{% if monitoring_additional_ingress_annotations %}
{% for key, value in monitoring_additional_ingress_annotations.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}

{% if monitoring_alertmanager_ingress_basic_auth_password %}
      nginx.ingress.kubernetes.io/auth-type: basic
      nginx.ingress.kubernetes.io/auth-secret: alertmanager-basic-auth

  extraSecret:
    name: alertmanager-basic-auth
    data:
      auth: |
        {{ monitoring_alertmanager_ingress_basic_auth_user }}:{{ monitoring_alertmanager_ingress_basic_auth_password | string | password_hash('bcrypt', salt=monitoring_alertmanager_ingress_basic_auth_password_salt) }}
{% endif %}

  alertmanagerSpec:
    logLevel: debug
    externalUrl: {{ 'https' if monitoring_alertmanager_ingress_tls else 'http' }}://{{ monitoring_alertmanager_ingress_dns }}
    containers:
    - name: webhook-logger
      image: {{ webhook_logger_image_name }}:{{ webhook_logger_image_tag }}
      ports:
      - containerPort: 8000
        name: webhook-logger
      readinessProbe:
        httpGet:
          path: /ready
          port: webhook-logger
  config:
    global:
      resolve_timeout: 5m
{% if monitoring_slack_api_url is defined %}
      slack_api_url: "{{ monitoring_slack_api_url }}"
{% endif %}
    route:
      receiver: webhook-logger
      group_by: ['severity', 'alertname', 'namespace', 'pod', 'job']
      group_wait: 30s
      group_interval: 30s
      repeat_interval: 600s
      routes:
      - receiver: 'null'
        matchers:
          - alertname=~"Watchdog|InfoInhibitor"
{% if monitoring_slack_api_url is defined and monitoring_slack_notification_channel is defined %}
      - receiver: slack
        repeat_interval: 3h
        continue: true
{% endif %}
{% for route in monitoring_alertmanager_additional_routes %}
      - {{ route | to_yaml | indent(width=8, first=false) }}
{% endfor %}
    receivers:
    - name: webhook-logger
      webhook_configs:
      - url: http://localhost:8000/alerts
        send_resolved: true
        max_alerts: 0 # 0=all alerts
    - name: 'null'
{% if monitoring_slack_api_url is defined and monitoring_slack_notification_channel is defined %}
    - name: slack
      slack_configs:
      - channel: "{{ monitoring_slack_notification_channel }}"
{% raw %}
        color: '{{ template "slack.color" . }}'
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        send_resolved: true
        actions:
          - type: button
            text: 'Runbook :green_book:'
            url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
          - type: button
            text: 'Query :mag:'
            url: '{{ (index .Alerts 0).GeneratorURL }}'
          - type: button
            text: 'Dashboard :chart_with_upwards_trend:'
            url: '{{ (index .Alerts 0).Annotations.dashboard_url }}'
          - type: button
            text: 'Silence :no_bell:'
            url: '{{ template "__alert_silence_link" . }}'
{% endraw %}
{% endif %}
{% for receiver in monitoring_alertmanager_additional_receivers %}
    - {{ receiver | to_yaml | indent(width=6, first=false) }}
{% endfor %}

{% raw %}
  templateFiles:
    slack.tmpl: |-
      {{/* taken from https://hodovi.cc/blog/creating-awesome-alertmanager-templates-for-slack/ */}}
      {{/* Alertmanager Silence link */}}
      {{ define "__alert_silence_link" -}}
          {{ .ExternalURL }}/#/silences/new?filter=%7B
          {{- range .CommonLabels.SortedPairs -}}
              {{- if ne .Name "alertname" -}}
                  {{- .Name }}%3D"{{- .Value -}}"%2C%20
              {{- end -}}
          {{- end -}}
          alertname%3D"{{- .CommonLabels.alertname -}}"%7D
      {{- end }}

      {{/* Severity of the alert */}}
      {{ define "__alert_severity" -}}
          {{- if eq .CommonLabels.severity "critical" -}}
          *Severity:* `Critical`
          {{- else if eq .CommonLabels.severity "warning" -}}
          *Severity:* `Warning`
          {{- else if eq .CommonLabels.severity "info" -}}
          *Severity:* `Info`
          {{- else -}}
          *Severity:* :question: {{ .CommonLabels.severity }}
          {{- end }}
      {{- end }}

      {{/* Title of the Slack alert */}}
      {{ define "slack.title" -}}
        [{{ .Status | toUpper -}}
        {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
        ] {{ .CommonLabels.alertname }}
      {{- end }}

      {{/* Color of Slack attachment (appears as line next to alert )*/}}
      {{ define "slack.color" -}}
          {{ if eq .Status "firing" -}}
              {{ if eq .CommonLabels.severity "warning" -}}
                  warning
              {{- else if eq .CommonLabels.severity "critical" -}}
                  danger
              {{- else -}}
                  #439FE0
              {{- end -}}
          {{ else -}}
          good
          {{- end }}
      {{- end }}

      {{/* The text to display in the alert */}}
      {{ define "slack.text" -}}

          {{ template "__alert_severity" . }}
          {{- if (index .Alerts 0).Annotations.summary }}
          {{- "\n" -}}
          *Summary:* {{ (index .Alerts 0).Annotations.summary }}
          {{- end }}

          {{ range .Alerts }}

              {{- if .Annotations.description }}
              {{- "\n" -}}
              {{ .Annotations.description }}
              {{- "\n" -}}
              {{- end }}
              {{- if .Annotations.message }}
              {{- "\n" -}}
              {{ .Annotations.message }}
              {{- "\n" -}}
              {{- end }}

          {{- end }}

      {{- end }}
{% endraw %}

coreDns:
  enabled: {{ monitoring_prometheus_core_dns_enabled }}

kubeDns:
  enabled: {{ monitoring_prometheus_kube_dns_enabled }}

kubeProxy:
  enabled: {{ monitoring_prometheus_kube_proxy_enabled }}

kubeScheduler:
  enabled: {{ monitoring_prometheus_kube_scheduler_enabled }}

kubeEtcd:
  enabled: {{ monitoring_prometheus_kube_etcd_enabled }}

kubeControllerManager:
  enabled: {{ monitoring_prometheus_kube_controller_manager_enabled }}

{% raw %}
additionalPrometheusRulesMap:
  "metal-alertmanager.rules":
    groups:
    - name: metal-api.rules
      rules:
      - alert: PrometheusErrorSendingAlertsToAlertmanager
        annotations:
          description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus
            {{ $labels.instance }} to Alertmanager {{ $labels.alertmanager }}.'
          summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
        expr: |
          (
            rate(prometheus_notifications_errors_total{job="prometheus"}[5m])
          /
            rate(prometheus_notifications_sent_total{job="prometheus"}[5m])
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: critical
  "metal-api.rules":
    groups:
    - name: metal-api.rules
      rules:
      - alert: MetalApiErrorRate
        expr: sum(rate(metal_api_requests_total{code!="200"}[5m])) by(pod)  / sum(rate(metal_api_requests_total[5m])) by(pod) *100 > 0.5
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "{{ $value | printf `%.2f` }}% of requests to {{ $labels.pod }} have errors"
      - alert: MetalApiSlow
        expr: histogram_quantile(0.90, sum by(le, pod) (rate(metal_api_request_duration_seconds_bucket{route="/metal/v1/machine/{id}/event"}[5m]))) > 0.5
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "Metal API has a 99th percentile latency of {{ $value | printf `%.2f` }} seconds for pod {{ $labels.pod }}"
      - alert: MetalApiGoProcessesHigh
        expr: avg(go_goroutines{job="metal-api"}) by(pod)>500
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "Metal API pod {{ $labels.pod }} has high number of go processes {{ $value | printf `%.2f` }}"
      - alert: MetalApiResponseCodeNotOk
        expr: http_requests_total{job="metal_api", status!~"2..|3.."} > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Metal API response status code not OK"
          description: "The metal_api response status code is not in the expected range."
      - alert: MetalFailedMachineReclaim
        expr: (metal_machine_issues{issueid="failed-machine-reclaim"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 90m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` did not go back into waiting machine pool after freeing the machine. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
      - alert: MetalMachineHasNoEventContainer
        expr: (metal_machine_issues{issueid="no-event-container"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` has no event container. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
      - alert: MetalBmcInfoOutdated
        expr: (metal_machine_issues{issueid="bmc-info-outdated"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` does not receive BMC updates. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
      - alert: MetalBmcNoDistinctIP
        expr: (metal_machine_issues{issueid="bmc-no-distinct-ip"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` has no distinct BMC address. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
      - alert: MetalMachineCrashloop
        expr: (metal_machine_issues{issueid="crashloop"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` is in a provisioning crashloop. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
      - alert: MetalMachineDead
        expr: (metal_machine_issues{issueid="liveliness-dead"} == 1) * on (machineid) group_left(partition,state) metal_machine_allocation_info{state!="LOCKED"} * on (issueid) group_left(description,severity,refurl) metal_machine_issues_info
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "Machine `{{ $labels.machineid }}` in partition `{{ $labels.partition }}` is dead. Use `metalctl machine issues` for further inspection ({{ $labels.refurl }}). The issue is not reported if an operator locks this machine."
    - name: metal-api-recording.rules
      rules:
      - record: frr:instance:metal_switch_interface_info
        expr: |2
          label_replace(metal_switch_interface_info,"instance","$1","switchname","(.*)")
  "partition.rules":
    groups:
    - name: alert.rules
      rules:
      - alert: DeadMachines
        expr: avg(metal_machine_liveliness_total{status="dead"}) by (partition) > 0
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "Partition {{ $labels.partition }} has {{ $value }} dead machines."
      - alert: MachineCapacityLow
        expr: (avg(metal_partition_capacity_free{size!="unknown"} > 5) by (partition, size) / avg(metal_partition_capacity_total{size!="unknown"} > 5) by (partition, size)) * 100 < 10
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "Running out of machines. Less than 10% free machines of size {{ $labels.size }} in {{ $labels.partition }}. (only {{ $value }}% left)"
      - alert: InternetIPCapacityLow
        expr: avg(metal_network_ip_used{networkId=~"internet.*"}) by (partition, networkId) / avg(metal_network_ip_available{networkId=~"internet.*"}) by (partition, networkId) * 100 > 80
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "{{ $value }}% of {{ $labels.networkId }} Internet IP addresses in {{ $labels.partition }} are in use."
      - alert: NetworkPrefixCapacityLow
        expr: avg(metal_network_prefix_used{isPrivateSuper="true"}) by (partition, networkId) / avg(metal_network_prefix_available{isPrivateSuper="true"}) by (partition, networkId)  * 100 > 80
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "{{ $value }}% of {{ $labels.networkId }} network prefixes in {{ $labels.partition }} are in use."
  "switch.rules":
    groups:
    - name: switch.rules
      rules:
      - alert: SwitchMetalCoreDown
        expr: metal_switch_metal_core_up == 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "metal-core is down on {{ $labels.switchname }}"
          description: "No syncs happening by metal-core on {{ $labels.switchname }} in partition {{ $labels.partition }} for more than 10 minutes."
      - alert: SwitchSyncFailing
        expr: metal_switch_sync_failed != 0
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Switch sync failing on {{ $labels.switchname }}"
          description: "{{ $labels.switchname }} in partition {{ $labels.partition }} is failing for more than 10 minutes."
      - alert: SwitchSyncSlow
        expr: avg by (switchname) (metal_switch_sync_durations_ms) > 2000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Switch sync is slow on {{ $labels.switchname }}"
          description: "Average sync duration of {{ $labels.switchname }} in partition {{ $labels.partition }} is exceeding two seconds for more than 5 minutes."
{% endraw %}
{% if monitoring_gardener_enabled %}
{% raw %}
  "gardener.rules":
    groups:
    - name: garden_seed_operation_states.rules
      rules:
      - alert: SeedConditionError
        expr: garden_seed_condition == 0
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "Seed {{ $labels.name }} condition {{ $labels.condition }} is in FAILED state"
      - alert: SeedEveryNodeReady
        expr: avg(garden_shoot_condition{condition="EveryNodeReady",purpose="infrastructure"}) by (name, condition) == 0
        for: 20m
        labels:
          severity: critical
        annotations:
          description: "Seed {{ $labels.name }} condition {{ $labels.condition }} is in FAILED state"
      - alert: SeedSystemComponentsHealthy
        expr: avg(garden_shoot_condition{condition="SystemComponentsHealthy",purpose="infrastructure"}) by (name, condition) == 0
        for: 20m
        labels:
          severity: critical
        annotations:
          description: "Seed {{ $labels.name }} condition {{ $labels.condition }} is in FAILED state"
    - name: garden_shoot_operation_states.rules
      rules:
      - alert: ShootOperationError
        expr: garden_shoot_operation_states + on(name, project) group_left(seed) avg(garden_shoot_info) by (name,project,seed) > 1
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: "{{ $labels.operation }} for cluster {{ $labels.name }} in project {{ $labels.project }} did not succeed."
          description: "Gardener could not {{ $labels.operation }} cluster {{ $labels.name }} for 30 minutes"
    - name: garden_shoot_condition_states.rules
      rules:
      - alert: ShootConditionError
        expr: garden_shoot_condition{operation="Reconcile"} == 0
        for: 15m
        labels:
          severity: warning
          type: shoot
        annotations:
          summary: "{{ $labels.condition }} for cluster {{ $labels.name }} in project {{ $labels.project }} is in FAILED state"
          description: "{{ $labels.condition }} for cluster {{ $labels.name }} in project {{ $labels.project }} is in FAILED state"
      - record: garden_shoot_condition:namespaced
        expr: label_join(label_replace(garden_shoot_condition,"namespace1","shoot--$1--","project","(.*)"),"namespace","","namespace1","name")
{% endraw %}
{% endif %}
