---
prometheus:
  ingress:
    enabled: {{ monitoring_prometheus_ingress_enabled }}
    ingressClassName: nginx
    hosts:
      - {{ monitoring_prometheus_ingress_dns }}
{% if monitoring_additional_ingress_annotations %}
    annotations:
{% for key, value in monitoring_additional_ingress_annotations.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}

  prometheusSpec:
{% if monitoring_prometheus_image_tag is defined %}
    image:
      tag: "{{ monitoring_prometheus_image_tag }}"
{% endif %}
{% if monitoring_prometheus_storage_spec is defined %}
    storageSpec:
      {{ monitoring_prometheus_storage_spec | indent(6) }}
{% endif %}
    thanos:
      image: "{{ thanos_image_name }}:{{ thanos_image_tag }}"
      version: "{{ thanos_image_tag }}"
{% if monitoring_thanos_object_store_config is defined %}
      objectStorageConfig:
        key: objstore.yml
        name: thanos-objstore-secret
{% endif %}
  thanosService:
    enabled: true

grafana:
  adminPassword: {{ monitoring_grafana_admin_password }}
  defaultDashboardsTimezone: {{ monitoring_grafana_dashboard_timezone }}

  additionalDataSources: {{ monitoring_grafana_additional_datasources | to_json }}
  grafana.ini:
    server:
{% if monitoring_ingress_grafana_tls %}
      root_url: https://{{ monitoring_grafana_ingress_dns }}
{% else %}
      root_url: http://{{ monitoring_grafana_ingress_dns }}
{% endif %}
{% if monitoring_grafana_github_oauth %}
    auth.github: {{ monitoring_grafana_github_oauth | to_json }}
{% endif %}
{% if monitoring_grafana_extra_secret_mounts %}
  extraSecretMounts: {{ monitoring_grafana_extra_secret_mounts | to_json }}
{% endif %}
  sidecar:
    datasources:
      uid: prometheus
      url: http://thanos-query-frontend:9090
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - {{ monitoring_grafana_ingress_dns }}
{% if monitoring_ingress_grafana_tls %}
    tls:
      - secretName: monitoring-ingress-tls
        hosts:
        - {{ monitoring_grafana_ingress_dns }}
{% endif %}
{% if monitoring_ingress_grafana_tls or monitoring_additional_ingress_annotations %}
    annotations:
{% endif %}
{% if monitoring_ingress_grafana_tls %}
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
{% endif %}
{% if monitoring_additional_ingress_annotations %}
{% for key, value in monitoring_additional_ingress_annotations.items() %}
      {{ key }}: "{{ value }}"
{% endfor %}
{% endif %}

alertmanager:
  alertmanagerSpec:
    logLevel: debug
    containers:
    - name: webhook-logger
      image: {{ webhook_logger_image_name }}:{{ webhook_logger_image_tag }}
      ports:
      - containerPort: 8000
        name: webhook-logger
      readinessProbe:
        httpGet:
          path: /ready
          port: webhook-logger
  config:
    global:
      resolve_timeout: 5m
{% if monitoring_slack_api_url is defined %}
      slack_api_url: "{{ monitoring_slack_api_url }}"
{% endif %}
    inhibit_rules: []
    route:
      receiver: webhook-logger
      group_by: ['severity', 'alertname', 'namespace', 'pod', 'job']
      group_wait: 30s
      group_interval: 30s
      repeat_interval: 30s
      routes:
      - receiver: 'null'
        matchers:
          - alertname=~"Watchdog|InfoInhibitor"
{% if monitoring_slack_api_url is defined and monitoring_slack_notification_channel is defined %}
      - receiver: slack
        repeat_interval: 1h
        continue: true
{% endif %}
    receivers:
    - name: webhook-logger
      webhook_configs:
      - url: http://localhost:8000/alerts
        send_resolved: true
        max_alerts: 0 # 0=all alerts
    - name: 'null'
{% if monitoring_slack_api_url is defined and monitoring_slack_notification_channel is defined %}
    - name: slack
      slack_configs:
{% raw %}
      - channel: "{{ monitoring_slack_notification_channel }}"
        color: '{{ template "slack.color" . }}'
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
        send_resolved: true
        actions:
          - type: button
            text: 'Runbook :green_book:'
            url: '{{ (index .Alerts 0).Annotations.runbook_url }}'
          - type: button
            text: 'Query :mag:'
            url: '{{ (index .Alerts 0).GeneratorURL }}'
          - type: button
            text: 'Dashboard :chart_with_upwards_trend:'
            url: '{{ (index .Alerts 0).Annotations.dashboard_url }}'
          - type: button
            text: 'Silence :no_bell:'
            url: '{{ template "__alert_silence_link" . }}'
{% endraw %}
{% endif %}

{% raw %}
  templateFiles:
    slack.tmpl: |-
      {{/* taken from https://hodovi.cc/blog/creating-awesome-alertmanager-templates-for-slack/ */}}
      {{/* Alertmanager Silence link */}}
      {{ define "__alert_silence_link" -}}
          {{ .ExternalURL }}/#/silences/new?filter=%7B
          {{- range .CommonLabels.SortedPairs -}}
              {{- if ne .Name "alertname" -}}
                  {{- .Name }}%3D"{{- .Value -}}"%2C%20
              {{- end -}}
          {{- end -}}
          alertname%3D"{{- .CommonLabels.alertname -}}"%7D
      {{- end }}

      {{/* Severity of the alert */}}
      {{ define "__alert_severity" -}}
          {{- if eq .CommonLabels.severity "critical" -}}
          *Severity:* `Critical`
          {{- else if eq .CommonLabels.severity "warning" -}}
          *Severity:* `Warning`
          {{- else if eq .CommonLabels.severity "info" -}}
          *Severity:* `Info`
          {{- else -}}
          *Severity:* :question: {{ .CommonLabels.severity }}
          {{- end }}
      {{- end }}

      {{/* Title of the Slack alert */}}
      {{ define "slack.title" -}}
        [{{ .Status | toUpper -}}
        {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
        ] {{ .CommonLabels.alertname }}
      {{- end }}

      {{/* Color of Slack attachment (appears as line next to alert )*/}}
      {{ define "slack.color" -}}
          {{ if eq .Status "firing" -}}
              {{ if eq .CommonLabels.severity "warning" -}}
                  warning
              {{- else if eq .CommonLabels.severity "critical" -}}
                  danger
              {{- else -}}
                  #439FE0
              {{- end -}}
          {{ else -}}
          good
          {{- end }}
      {{- end }}

      {{/* The text to display in the alert */}}
      {{ define "slack.text" -}}

          {{ template "__alert_severity" . }}
          {{- if (index .Alerts 0).Annotations.summary }}
          {{- "\n" -}}
          *Summary:* {{ (index .Alerts 0).Annotations.summary }}
          {{- end }}

          {{ range .Alerts }}

              {{- if .Annotations.description }}
              {{- "\n" -}}
              {{ .Annotations.description }}
              {{- "\n" -}}
              {{- end }}
              {{- if .Annotations.message }}
              {{- "\n" -}}
              {{ .Annotations.message }}
              {{- "\n" -}}
              {{- end }}

          {{- end }}

      {{- end }}
{% endraw %}

coreDns:
  enabled: false

kubeDns:
  enabled: true

kubeProxy:
  enabled: false

kubeScheduler:
  enabled: false

kubeEtcd:
  enabled: false

kubeControllerManager:
  enabled: false

{% raw %}
additionalPrometheusRulesMap:
  "metal-api.rules":
    groups:
    - name: metal-api.rules
      rules:
      - alert: MetalApiErrorRate
        expr: sum(rate(metal_api_requests_total{code!="200"}[5m])) by(pod)  / sum(rate(metal_api_requests_total[5m])) by(pod) *100 > 0.5
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "{{ $value | printf `%.2f` }}% of requests to {{ $labels.pod }} have errors"
      - alert: MetalApiSlow
        expr: histogram_quantile(0.90, sum by(le, pod) (rate(metal_api_request_duration_seconds_bucket{route="/metal/v1/machine/{id}/event"}[5m]))) > 0.5
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "Metal API has a 99th percentile latency of {{ $value | printf `%.2f` }} seconds for pod {{ $labels.pod }}"
      - alert: MetalApiGoProcessesHigh
        expr: avg(go_goroutines{job="metal-api"}) by(pod)>500
        for: 30m
        labels:
          severity: critical
        annotations:
          description: "Metal API pod {{ $labels.pod }} has high number of go processes {{ $value | printf `%.2f` }}"
      - alert: MetalApiResponseCodeNotOk
        expr: http_requests_total{job="metal_api", status!~"2..|3.."} > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Metal API response status code not OK"
          description: "The metal_api response status code is not in the expected range."
    - name: metal-api-recording.rules
      rules:
      - record: frr:instance:metal_switch_interface_info
        expr: |2
          label_replace(metal_switch_interface_info,"instance","$1","switchname","(.*)")
  "partition.rules":
    groups:
    - name: alert.rules
      rules:
      - alert: DeadMachines
        expr: avg(metal_machine_liveliness_total{status="dead"}) by (partition) > 0
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "Partition {{ $labels.partition }} has {{ $value }} DEAD Machines"
      - alert: MachineCapacityLow
        expr: (avg(metal_partition_capacity_free{size!="unknown"} > 5) by (partition, size) / avg(metal_partition_capacity_total{size!="unknown"} > 5) by (partition, size)) * 100 < 80
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "Running out of machines. Less than 80% free machines of size {{ $labels.size }} in {{ $labels.partition }}. (only {{ $value }}% are free)"
      - alert: InternetIPCapacityLow
        expr: avg(metal_network_ip_used{networkId=~"internet.*"}) by (partition, networkId) / avg(metal_network_ip_available{networkId=~"internet.*"}) by (partition, networkId) * 100 > 80
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "{{ $value }}% of {{ $labels.networkId }} Internet IP adresses in {{ $labels.partition }} are in use."
      - alert: NetworkPrefixCapacityLow
        expr: avg(metal_network_prefix_used{isPrivateSuper="true"}) by (partition, networkId) / avg(metal_network_prefix_available{isPrivateSuper="true"}) by (partition, networkId)  * 100 > 80
        for: 10m
        labels:
          severity: "warning"
        annotations:
          description: "{{ $value }}% of {{ $labels.networkId }} network prefixes in {{ $labels.partition }} are in use."
{% endraw %}
